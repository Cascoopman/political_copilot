name: parlementaire-vragen
services:
  fetchlinks:
    command:
    - --metadata
    - '{"base_path": "/fondant-artifacts", "pipeline_name": "parlementaire-vragen",
      "run_id": "parlementaire-vragen-20240311140556", "component_id": "fetchlinks",
      "cache_key": "38f4f5a6b581bc2de312998a91c4b19f"}'
    - --output_manifest_path
    - /fondant-artifacts/parlementaire-vragen/parlementaire-vragen-20240311140556/fetchlinks/manifest.json
    - --link
    - https://www.vlaamsparlement.be/ajax/document-overview?page=0&period=current_year_of_office&current_year_of_office_value=2022-2023&aggregaat%5B%5D=Vraag%20of%20interpellatie&aggregaattype%5B%5D=Schriftelijke%20vraag&thema%5B%5D=Natuur%20en%20Milieu
    - --cache
    - 'True'
    - --cluster_type
    - default
    - --operation_spec
    - '{"specification": {"name": "FetchLinks", "image": "fndnt/fondant:0.10.1-py3.10",
      "description": "lightweight component", "consumes": {"additionalProperties":
      true}, "produces": {"File Name": {"type": "string"}, "Card title": {"type":
      "string"}, "Document number": {"type": "string"}, "View link text": {"type":
      "string"}, "View link href": {"type": "string"}, "Download link text": {"type":
      "string"}, "Download link href": {"type": "string"}}, "args": {"link": {"type":
      "str"}}}, "consumes": {}, "produces": {}}'
    depends_on: {}
    entrypoint:
    - sh
    - -ec
    - "                printf 'requests\nbs4' > 'requirements.txt'\n             \
      \   python3 -m pip install -r requirements.txt\n            printf 'from typing\
      \ import *\nimport typing as t\nimport requests\nfrom bs4 import BeautifulSoup\n\
      import re\nimport dask.dataframe as dd\nimport fondant\nimport pandas as pd\n\
      from fondant.component import *\nfrom fondant.core import *\n\n\nclass FetchLinks(DaskLoadComponent):\n\
      \    def __init__(self, link: str):\n        self.link = link\n\n    def get_link(self):\n\
      \        return self.link\n\n    def load(self) -> dd.DataFrame:\n        AMOUNT_OF_PAGES\
      \ = 1\n        data = []\n\n        for page_number in range(0, AMOUNT_OF_PAGES):\n\
      \            data.extend(self.fetch_document_info(page_number))\n\n        df\
      \ = pd.DataFrame(data)\n        return dd.from_pandas(df, npartitions=1)\n\n\
      \    def fetch_document_info(self, page_number):\n        data = []\n      \
      \  request_URL = self.get_link()\n        headers = {'\"'\"'Accept'\"'\"': '\"\
      '\"'application/json'\"'\"'}\n        response = requests.get(request_URL, headers=headers)\n\
      \n        json_content = response.json()\n        html_content = json_content['\"\
      '\"'html'\"'\"']\n\n        soup = BeautifulSoup(html_content, '\"'\"'html.parser'\"\
      '\"')\n\n        # Find all articles with class \"card card--document\"\n  \
      \      articles = soup.find_all('\"'\"'article'\"'\"', class_='\"'\"'card card--document'\"\
      '\"')\n        i = 0\n        # Loop through each article\n        for article\
      \ in articles:\n            i += 1 \n            # Extract the card title\n\
      \            card_title = article.find('\"'\"'h3'\"'\"', class_='\"'\"'card__title'\"\
      '\"').get_text()\n\n            # Extract the document number\n            doc_number\
      \ = article.find('\"'\"'span'\"'\"', class_='\"'\"'card__document-number'\"\
      '\"').get_text()\n\n            # Find the link with class \"card__link card__link-view\"\
      \n            view_link = article.find('\"'\"'li'\"'\"', class_='\"'\"'card__link\
      \ card__link-view'\"'\"')\n            # Find the link with class \"card__link\
      \ card__link-download\"\n            download_link = article.find('\"'\"'li'\"\
      '\"', class_='\"'\"'card__link card__link-download'\"'\"')\n\n            #\
      \ Extract the text and href attribute from the view link, if it exists     \
      \   \n            if view_link:\n                view_text = view_link.get_text()\n\
      \                view_href = view_link.find('\"'\"'a'\"'\"')['\"'\"'href'\"\
      '\"']\n            else:\n                view_text, view_href = '\"'\"''\"\
      '\"', '\"'\"''\"'\"'\n\n            # Extract the text and href attribute from\
      \ the download link, if it exists\n            if download_link:\n         \
      \       download_text = download_link.get_text()\n                download_href\
      \ = download_link.find('\"'\"'a'\"'\"')['\"'\"'href'\"'\"']\n            else:\n\
      \                download_text, download_href = '\"'\"''\"'\"', '\"'\"''\"'\"\
      '\n\n            # add to dataframe\n            card_title = card_title.replace(\"\
      /\", \"_\") #otherwise it will try to find a folder\n            file_name =\
      \ f\"{card_title}_{doc_number}\"\n            data.append({\n              \
      \  '\"'\"'File Name'\"'\"': file_name,\n                '\"'\"'Card title'\"\
      '\"': card_title, \n                '\"'\"'Document number'\"'\"': doc_number,\
      \ \n                '\"'\"'View link text'\"'\"': view_text,\n             \
      \   '\"'\"'View link href'\"'\"': view_href, \n                '\"'\"'Download\
      \ link text'\"'\"': download_text, \n                '\"'\"'Download link href'\"\
      '\"': download_href\n                })\n\n        return data\n' > 'main.py'\n\
      \            fondant execute main \"$@\"\n"
    - --
    image: fndnt/fondant:0.10.1-py3.10
    labels:
      pipeline_description: Retrieve pdf download links, download content, structure
        the content and index into vector DB.
    ports:
    - 8787:8787
    volumes:
    - source: /Users/cas/political_agents/microservice/app/scrapers/ParlementaireVragen/fondant-artifacts
      target: /fondant-artifacts
      type: bind
version: '3.8'
